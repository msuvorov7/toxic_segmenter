import logging
import os
import sys

import compress_fasttext
import onnxruntime

from aiogram import Bot, types
from aiogram.dispatcher import Dispatcher
from aiogram.utils import executor

sys.path.insert(0, os.path.dirname(
    os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
))

from src.utils.transformer import FeatureTransformer

fileDir = os.path.join(os.path.dirname(os.path.realpath(__file__)), '../../')

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')

bot = Bot(token=TOKEN)
dp = Dispatcher(bot)


def get_result_message(text: str) -> str:
    """
    –ü–∞–π–ø–ª–∞–π–Ω –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—ã—Ä–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
    :param text: —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç
    :return: —Ü–µ–Ω–∑—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    transformer = FeatureTransformer(fasttext_model, model)
    tokens = transformer.tokenizer.tokenize(text)
    probabilities = transformer.predict(text)

    threshold = 0.2
    toxic_smile = 'ü§¨'
    censored_tokens = [tok if prob < threshold else toxic_smile for (tok, prob) in zip(tokens, probabilities)]

    return transformer.tokenizer.detokenize(censored_tokens)


@dp.message_handler(commands=['start'])
async def welcome_start(message):
    await message.answer('Hello')


@dp.message_handler(lambda message: message.caption is not None, content_types=['photo'])
async def parse_photo(message: types.message):
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—Ö–æ–¥—è—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è–º–∏
    :param message: —Ñ–æ—Ç–æ —Å –ø–æ–¥–ø–∏—Å—å—é
    :return: —Ü–µ–Ω–∑—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—è –ø–æ–¥–ø–∏—Å—å –∏ —Ñ–æ—Ç–æ
    """
    result_message = get_result_message(message.caption)
    print(message.caption)
    await message.answer_photo(photo=message.photo[-1].file_id, caption=result_message)


@dp.message_handler(content_types=['text'])
async def parse_text(message: types.message):
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—Ö–æ–¥—è—â–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    :param message: —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
    :return: —Ü–µ–Ω–∑—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
    """
    text = message.text
    print(text)
    result_message = get_result_message(text)
    await message.answer(text=result_message)


if __name__ == '__main__':
    model = onnxruntime.InferenceSession('../models/segmenter.onnx')
    logging.info(f'model loaded')

    fasttext_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load('../models/tiny_fasttext.model')
    logging.info(f'fasttext model loaded')

    executor.start_polling(dp, skip_updates=True)
