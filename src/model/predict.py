import argparse
import logging
import os
import sys

import compress_fasttext
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

import yaml

sys.path.insert(0, os.path.dirname(
    os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
))

from src.utils.preprocess_rules import Preprocessor
from src.data_load.create_dataframe import tokenize

fileDir = os.path.join(os.path.dirname(os.path.realpath(__file__)), '../../')

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)


def load_model(directory_path: str) -> nn.Module:
    """
    Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    :param directory_path: Ğ¸Ğ¼Ñ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸
    :return:
    """
    model_path = directory_path + 'model.torch'
    model = torch.load(model_path)
    return model


def predict(message: str) -> (list, list):
    tokens = tokenize(message)
    preprocessor = Preprocessor()
    cleaned_tokens = [preprocessor.forward(token) for token in tokens]
    encoded = [fasttext_model[item] for item in cleaned_tokens]

    prediction = model(torch.tensor(np.array(encoded)))
    prediction = prediction.view(-1, prediction.shape[2])

    preds = F.softmax(prediction, dim=1)[:, 1].cpu().detach().numpy()

    return tokens, preds


if __name__ == '__main__':
    args_parser = argparse.ArgumentParser()
    args_parser.add_argument('--config', default='params.yaml', dest='config')
    # args_parser.add_argument('--fasttext_name', default='fasttext_pretrained.model', dest='fasttext_name')
    args_parser.add_argument('--fasttext_name', default='tiny_fasttext.model', dest='fasttext_name')
    args = args_parser.parse_args()

    with open(fileDir + args.config) as conf_file:
        config = yaml.safe_load(conf_file)

    model = load_model(fileDir + config['models'])
    logging.info(f'model loaded')

    # fasttext_model = load_fasttext_model(fileDir + config['models'] + args.fasttext_name)
    fasttext_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load(fileDir + config['models'] + args.fasttext_name)
    logging.info(f'fasttext model loaded')

    text = """
    Ğ¿Ğ¸Ğ´Ñ€Ğ¸Ğ»Ğ° Ğ·Ğ»Ğ¾ĞµĞ±ÑƒÑ‡Ğ¸Ğ¹ ÑƒĞ±ĞµÑ€Ğ¸ ÑĞ²Ğ¾Ñ ÑĞ¼Ğ°Ğ·Ğ»Ğ¸Ğ²ÑƒÑ Ğ¼Ğ¾Ñ€Ğ´Ñƒ.
    ÑĞ¾Ğ±Ğ°ĞºĞ° ĞºĞ¾Ğ½Ñ‡ĞµĞ½Ğ½Ğ°Ñ Ğ²Ğ¾Ñ‚ Ñ‚Ñ‹ ĞºÑ‚Ğ¾.
    Ğ·Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ñ€Ğ¾Ğ´Ñƒ ÑÑ‚Ğ¸Ñ… Ñ…ÑƒĞµĞ¿Ğ»ĞµÑ‚Ğ¾Ğ².
    Ğ¼Ğ°Ğ·ÑŒ Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑŒ.
    ĞºĞ¾Ğ¿Ğ°Ñ‚ÑŒ Ğ½Ğµ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ.
    Ğ¼Ğ½Ğµ ĞºĞ°Ğ¶ĞµÑ‚ÑÑ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¸Ğ´Ğ°Ñ€Ğ¾Ğº ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ´Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚.
    ĞµĞ±@Ğ½ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ĞºÑ€Ğ¾Ğ¹, Ğ¿Ğ¸Ğ´Ñ€Ğº.
    Ğ¿Ğ¸Ğ·Ğ´Ğ°Ñ†Ğ¸Ñ€Ğº ĞºĞ°ĞºĞ¾Ğ¹-Ñ‚Ğ¾.
    """
    tokens, preds = predict(text)

    for token, pred in zip(tokens, preds):
        if pred > 0.5:
            print(f'ğŸ¤¬ {token}: {pred:.2f}')
        else:
            print(f'{token}: {pred:.2f}')
