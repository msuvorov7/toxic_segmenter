import unittest
import nltk
from src.utils.tokenizer import Tokenizer


class TestTokenizer(unittest.TestCase):

    texts = [
        '–µ–¥–≤–∞ –Ω–µ –∑..........–∞–æ—Ä–∞–ª:""–µ–±–∞—Ç—å-–∫–æ–ø–∞—Ç—å! –≠—Ç–æ –∂–µ —Å–Ω–µ–≥!""',
        '–î–∞ –∫–∞–∫ –∂–µ –æ–Ω –ó–ê*–ë–ê–õ!!!',
        '–ß–∏—Ç–∞–π –ø–æ –≥—É–±–∞–º: –ø–∏ –¥–∞ —Ä–∞—Å)',
        '–ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –æ–Ω —Å—Ç–∞–ª —Å–∞–º –Ω–µ —Å–≤–æ–π.–ø–æ—á–µ–º—É - —Ö–∑',
        'x\ne\nr\n',
        '–∏–∑-–∑–∞ —Ç–µ–±—è —á—Ç–æ - —Ç–æ –ø–æ—à–ª–æ –Ω–µ –ø–æ –ø–ª–∞–Ω—É',
        '—Å–º–æ—Ç—Ä–∏ —à—É—Ç–∫—É: 3.14–¥–æ—Ä',
        '–Ω–∞ü§¨ –≤–æ–∑—å–º–∏',
    ]

    def test_tokenize(self):
        print('-' * 50 + '\nBase Tokenizer\n' + '-' * 50)
        tokenizer = Tokenizer()

        for text in self.texts:
            print('|'.join(tokenizer.tokenize(text)))

        print('-' * 50 + '\nNLTK  WordPunctTokenizer\n' + '-' * 50)
        tokenizer = nltk.WordPunctTokenizer()

        for text in self.texts:
            print('|'.join(tokenizer.tokenize(text)))

        print('-' * 50 + '\nNLTK  TreebankWordTokenizer\n' + '-' * 50)
        tokenizer = nltk.TreebankWordTokenizer()

        for text in self.texts:
            print('|'.join(tokenizer.tokenize(text)))

        self.assertTrue(True)
